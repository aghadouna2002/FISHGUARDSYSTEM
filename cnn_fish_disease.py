# -*- coding: utf-8 -*-
"""CNN_FISH_DISEASE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PB_3BC5zWMCgA8egPlijv-OvButDJJMe
"""

# Cell 1: Setup & Imports
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input
import matplotlib.pyplot as plt
import numpy as np
import os
import random
from sklearn.utils import class_weight
from tensorflow.data import AUTOTUNE

# For reproducibility
seed = 42
os.environ['PYTHONHASHSEED'] = str(seed)
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)
print("TensorFlow version:", tf.__version__)

# Cell 2: Unzip and Directory Listing
import zipfile
zip_path = "/content/diseasedetection.zip"
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall("/content")

dataset_folder = "/content/diseasedetection/Freshwater Fish Disease Aquaculture in south asia"
train_dir = os.path.join(dataset_folder, 'Train')
test_dir  = os.path.join(dataset_folder, 'Test')
print("Train subfolders:", os.listdir(train_dir))
print("Test subfolders:", os.listdir(test_dir))

# Cell 3: Compute Class Counts & Class Weights
class_names = sorted(os.listdir(train_dir))
class_counts = {cls: len(os.listdir(os.path.join(train_dir, cls))) for cls in class_names}
print("Class counts:")
for cls, cnt in class_counts.items(): print(f"  {cls}: {cnt}")
total = sum(class_counts.values())
class_weights = {i: total/count for i, count in enumerate(class_counts.values())}
print("Class weights:", class_weights)

# Cell 4: Create Train/Validation/Test Datasets
IMG_SIZE = (224, 224)
BATCH_SIZE = 32
SEED = 42

# Create training and validation splits from the Train directory
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    train_dir,
    labels="inferred",
    label_mode="categorical",
    batch_size=BATCH_SIZE,
    image_size=IMG_SIZE,
    shuffle=True,
    seed=SEED,
    validation_split=0.2,
    subset="training"
)
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    train_dir,
    labels="inferred",
    label_mode="categorical",
    batch_size=BATCH_SIZE,
    image_size=IMG_SIZE,
    shuffle=True,
    seed=SEED,
    validation_split=0.2,
    subset="validation"
)

# Create test dataset without any split
test_ds = tf.keras.preprocessing.image_dataset_from_directory(
    test_dir,
    labels="inferred",
    label_mode="categorical",
    batch_size=BATCH_SIZE,
    image_size=IMG_SIZE,
    shuffle=False
)

# Preprocess inputs outside the model
def preprocess(image, label):
    image = tf.cast(image, tf.float32)
    image = preprocess_input(image)
    return image, label

train_ds = train_ds.map(preprocess, num_parallel_calls=AUTOTUNE)
val_ds = val_ds.map(preprocess,   num_parallel_calls=AUTOTUNE)
test_ds = test_ds.map(preprocess, num_parallel_calls=AUTOTUNE)

# Optimize pipelines
train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds   = val_ds.cache().prefetch(buffer_size=AUTOTUNE)
test_ds  = test_ds.cache().prefetch(buffer_size=AUTOTUNE)

# Print batch counts
print("Train batches:", tf.data.experimental.cardinality(train_ds).numpy())
print("Val batches:",   tf.data.experimental.cardinality(val_ds).numpy())
print("Test batches:",  tf.data.experimental.cardinality(test_ds).numpy())

# Cell 5: Data Augmentation Visualization (using raw images)
# Reload a small batch of raw images without preprocessing
viz_ds = tf.keras.preprocessing.image_dataset_from_directory(
    train_dir,
    labels="inferred",
    label_mode="categorical",
    batch_size=9,
    image_size=IMG_SIZE,
    shuffle=True,
    seed=SEED
)

plt.figure(figsize=(10,10))
for images, labels in viz_ds.take(1):
    aug_images = data_augmentation(images)
    for i in range(9):
        ax = plt.subplot(3,3,i+1)
        # images are uint8 [0,255], so cast after augmentation
        plt.imshow(aug_images[i].numpy().astype('uint8'))
        plt.title(class_names[np.argmax(labels[i])])
        plt.axis("off")
plt.suptitle("Sample Augmented Images", fontsize=16)
plt.show()

# Cell 6: Build the Model
inputs = layers.Input(shape=IMG_SIZE + (3,))
x = data_augmentation(inputs)

# Base model without preprocessing layer
base_model = EfficientNetB0(include_top=False, weights="imagenet", input_tensor=x)
base_model.trainable = False

x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dropout(0.2)(x)
outputs = layers.Dense(len(class_names), activation="softmax")(x)
model = models.Model(inputs, outputs)

# Cell 7: Compile & Summary
model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
model.summary()

# Cell 8: Train Top Layers
EPOCHS = 10
early_stop = callbacks.EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)
checkpoint_path = os.path.join(os.getcwd(), "best_fish_model.h5")
# The file extension .h5 infers HDF5 saving; no explicit save_format parameter needed
checkpoint = callbacks.ModelCheckpoint(
    checkpoint_path,
    monitor="val_loss",
    save_best_only=True
)

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    class_weight=class_weights,
    callbacks=[early_stop, checkpoint]
)
print("Checkpoint saved at", checkpoint_path)

# Cell 9: Fine-Tuning
def fine_tune(model, base_model, train_ds, val_ds):
    for layer in base_model.layers[-100:]: layer.trainable = True
    model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss="categorical_crossentropy", metrics=["accuracy"])
    return model.fit(
        train_ds,
        validation_data=val_ds,
        initial_epoch=history.epoch[-1],
        epochs=EPOCHS+5,
        class_weight=class_weights,
        callbacks=[early_stop, checkpoint]
    )
history_fine = fine_tune(model, base_model, train_ds, val_ds)

# Cell 10: Evaluation
from sklearn.metrics import confusion_matrix, classification_report
import itertools

best_model = tf.keras.models.load_model(checkpoint_path)
print("Loaded model from", checkpoint_path)

y_true, y_pred = [], []
for images, labels in test_ds:
    preds = best_model.predict(images)
    y_true.extend(np.argmax(labels, axis=1))
    y_pred.extend(np.argmax(preds, axis=1))

cm = confusion_matrix(y_true, y_pred)

def plot_cm(cm, classes):
    plt.figure(figsize=(8,8))
    plt.imshow(cm, cmap=plt.cm.Blues)
    plt.title("Confusion Matrix")
    plt.colorbar()
    ticks = np.arange(len(classes))
    plt.xticks(ticks, classes, rotation=45)
    plt.yticks(ticks, classes)
    thresh = cm.max()/2
    for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j,i,cm[i,j],ha="center", color="white" if cm[i,j]>thresh else "black")
    plt.ylabel("True label"); plt.xlabel("Predicted label"); plt.tight_layout(); plt.show()
plot_cm(cm, class_names)
print(classification_report(y_true, y_pred, target_names=class_names))

# Cell 11: Save Final Model
final_model_path = os.path.join(os.getcwd(), "final_fish_disease_model.keras")
best_model.save(final_model_path)
print("Model saved to", final_model_path)

# Cell 12: Inference on Custom Images
from tensorflow.keras.preprocessing import image

def predict_image(img_path):
    # Load and preprocess the image
    img = tf.keras.utils.load_img(img_path, target_size=IMG_SIZE)
    img_array = tf.keras.utils.img_to_array(img)
    img_array = preprocess_input(img_array)
    img_array = np.expand_dims(img_array, axis=0)

    # Perform prediction
    preds = best_model.predict(img_array)
    class_idx = np.argmax(preds, axis=1)[0]
    confidence = preds[0][class_idx]
    label = class_names[class_idx]

    # Display the image with predicted label
    plt.figure(figsize=(4, 4))
    plt.imshow(img)
    plt.title(f"{label} ({confidence:.2%} confidence)")
    plt.axis("off")
    plt.show()

    return label, confidence

# Example usage:
sample_img_path = "/content/diseasedetection/Freshwater Fish Disease Aquaculture in south asia/Test/Bacterial Red disease/Bacterial Red disease (1).jpeg"
predicted_label, conf = predict_image(sample_img_path)
print(f"Model prediction: {predicted_label} with confidence {conf:.2%}")

